{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import MLP1Base, MLP2Base, MetaMLPModel\n",
    "from torchmeta.modules import MetaLinear\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./parallelograms-revisited/experiment2_data_relational_similarity/mean_relsim_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['mean_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation1</th>\n",
       "      <th>relation2</th>\n",
       "      <th>comparison_type</th>\n",
       "      <th>pair1_word1</th>\n",
       "      <th>pair1_word2</th>\n",
       "      <th>pair2_word1</th>\n",
       "      <th>pair2_word2</th>\n",
       "      <th>mean_rating</th>\n",
       "      <th>num_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3a</td>\n",
       "      <td>3a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>candy</td>\n",
       "      <td>sweets</td>\n",
       "      <td>sofa</td>\n",
       "      <td>chair</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1a</td>\n",
       "      <td>1a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>dollar</td>\n",
       "      <td>currency</td>\n",
       "      <td>wheat</td>\n",
       "      <td>bread</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9a</td>\n",
       "      <td>9a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>store</td>\n",
       "      <td>shopper</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>groceries</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6a</td>\n",
       "      <td>6a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>classic</td>\n",
       "      <td>trendy</td>\n",
       "      <td>integrity</td>\n",
       "      <td>dishonest</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10a</td>\n",
       "      <td>10a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>flashing</td>\n",
       "      <td>caution</td>\n",
       "      <td>green</td>\n",
       "      <td>go</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189</th>\n",
       "      <td>8b</td>\n",
       "      <td>8a</td>\n",
       "      <td>between-subtype</td>\n",
       "      <td>embarrassed</td>\n",
       "      <td>blush</td>\n",
       "      <td>explosion</td>\n",
       "      <td>damage</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>1a</td>\n",
       "      <td>2a</td>\n",
       "      <td>between-type</td>\n",
       "      <td>animal</td>\n",
       "      <td>carabao</td>\n",
       "      <td>dog</td>\n",
       "      <td>tail</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6191</th>\n",
       "      <td>5a</td>\n",
       "      <td>7c</td>\n",
       "      <td>between-type</td>\n",
       "      <td>intellectual</td>\n",
       "      <td>professor</td>\n",
       "      <td>police</td>\n",
       "      <td>gun</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>8a</td>\n",
       "      <td>8a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>education</td>\n",
       "      <td>expertise</td>\n",
       "      <td>showering</td>\n",
       "      <td>cleanliness</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>10a</td>\n",
       "      <td>10a</td>\n",
       "      <td>within-subtype</td>\n",
       "      <td>cross</td>\n",
       "      <td>faith</td>\n",
       "      <td>wrinkle</td>\n",
       "      <td>age</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6194 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     relation1 relation2  comparison_type   pair1_word1 pair1_word2  \\\n",
       "0           3a        3a   within-subtype         candy      sweets   \n",
       "1           1a        1a   within-subtype        dollar    currency   \n",
       "2           9a        9a   within-subtype         store     shopper   \n",
       "3           6a        6a   within-subtype       classic      trendy   \n",
       "4          10a       10a   within-subtype      flashing     caution   \n",
       "...        ...       ...              ...           ...         ...   \n",
       "6189        8b        8a  between-subtype   embarrassed       blush   \n",
       "6190        1a        2a     between-type        animal     carabao   \n",
       "6191        5a        7c     between-type  intellectual   professor   \n",
       "6192        8a        8a   within-subtype     education   expertise   \n",
       "6193       10a       10a   within-subtype         cross       faith   \n",
       "\n",
       "      pair2_word1  pair2_word2  mean_rating  num_ratings  \n",
       "0            sofa        chair     3.750000           12  \n",
       "1           wheat        bread     4.727273           11  \n",
       "2     supermarket    groceries     5.583333           12  \n",
       "3       integrity    dishonest     3.700000           10  \n",
       "4           green           go     6.583333           12  \n",
       "...           ...          ...          ...          ...  \n",
       "6189    explosion       damage     5.200000           10  \n",
       "6190          dog         tail     3.090909           11  \n",
       "6191       police          gun     3.200000           10  \n",
       "6192    showering  cleanliness     4.800000           10  \n",
       "6193      wrinkle          age     4.600000           10  \n",
       "\n",
       "[6194 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_linear_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan15_analogy/2021-01-15_091502linear/model.th', map_location=torch.device('cpu'))\n",
    "# meta_mlp1_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan15_analogy/2021-01-15_091502mlp1/model.th', map_location=torch.device('cpu'))\n",
    "# meta_mlp2_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan15_analogy/2021-01-15_091502mlp2/model.th', map_location=torch.device('cpu'))\n",
    "\n",
    "meta_linear_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan20_analogy/2021-01-20_093635linear/model.th', map_location=torch.device('cpu'))\n",
    "meta_mlp1_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan20_analogy/2021-01-20_093635mlp1/model.th', map_location=torch.device('cpu'))\n",
    "meta_mlp2_params = torch.load('/n/fs/nlp-myhu/research-code/pytorch-maml/output/jan20_analogy/2021-01-20_093635mlp2/model.th', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_linear = nn.Linear(in_features=300, out_features=300)\n",
    "meta_mlp1 = MLP1Base(input_dim=300, hidden_dim=500, output_dim=300)\n",
    "meta_mlp2 = MLP2Base(input_dim=300, hidden_dim=500, output_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_linear.weight.data = meta_linear_params['weight']\n",
    "meta_linear.bias.data = meta_linear_params['bias']\n",
    "\n",
    "meta_mlp1.l1.weight.data = meta_mlp1_params['features.layer1.linear.weight']\n",
    "meta_mlp1.l1.bias.data = meta_mlp1_params['features.layer1.linear.bias']\n",
    "meta_mlp1.l2.weight.data = meta_mlp1_params['classifier.weight']\n",
    "meta_mlp1.l2.bias.data = meta_mlp1_params['classifier.bias']\n",
    "\n",
    "meta_mlp2.l1.weight.data = meta_mlp2_params['features.layer1.linear.weight']\n",
    "meta_mlp2.l1.bias.data = meta_mlp2_params['features.layer1.linear.bias']\n",
    "meta_mlp2.l2.weight.data = meta_mlp2_params['features.layer2.linear.weight']\n",
    "meta_mlp2.l2.bias.data = meta_mlp2_params['features.layer2.linear.bias']\n",
    "meta_mlp2.l3.weight.data = meta_mlp2_params['classifier.weight']\n",
    "meta_mlp2.l3.bias.data = meta_mlp2_params['classifier.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(df, vocab, get_loss):\n",
    "    prediction_dict = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        w1, w2, w3, w4 = row['pair1_word1'], row['pair1_word2'], row['pair2_word1'], row['pair2_word2']\n",
    "\n",
    "        if w1 not in vocab.stoi or w2 not in vocab.stoi or w3 not in vocab.stoi or w4 not in vocab.stoi:\n",
    "            continue\n",
    "            \n",
    "        category_id = int(row['relation1'][:-1])\n",
    "        human_score = (row['mean_rating'] - 1) / 6 + 1e-20\n",
    "            \n",
    "        a = vocab[w1]\n",
    "        b = vocab[w2]\n",
    "        c = vocab[w3]\n",
    "        d = vocab[w4]\n",
    "        \n",
    "        forward = get_loss(a, b, c, d)\n",
    "\n",
    "        prediction_dict.append(-np.log(human_score)/forward)\n",
    "        \n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorized_relsim_scores(df, vocab, get_loss, alpha=10):\n",
    "    prediction_dict = defaultdict(lambda: [])\n",
    "    word_dict = defaultdict(lambda: [])\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        w1, w2, w3, w4 = row['pair1_word1'], row['pair1_word2'], row['pair2_word1'], row['pair2_word2']\n",
    "\n",
    "        if w1 not in vocab.stoi or w2 not in vocab.stoi or w3 not in vocab.stoi or w4 not in vocab.stoi:\n",
    "            continue\n",
    "            \n",
    "        category_id = int(row['relation1'][:-1])\n",
    "        human_score = row['mean_rating']\n",
    "        word_dict[category_id].append(human_score)\n",
    "            \n",
    "        a = vocab[w1]\n",
    "        b = vocab[w2]\n",
    "        c = vocab[w3]\n",
    "        d = vocab[w4]\n",
    "        \n",
    "        forward = get_loss(a, b, c, d)\n",
    "        backward = get_loss(c, d, a, b)\n",
    "\n",
    "        prediction_dict[category_id].append(np.exp(-alpha * forward))\n",
    "        \n",
    "    return prediction_dict, word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorized_relsim_scores_cos(df, vocab, get_loss):\n",
    "    prediction_dict = defaultdict(lambda: [])\n",
    "    word_dict = defaultdict(lambda: [])\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        w1, w2, w3, w4 = row['pair1_word1'], row['pair1_word2'], row['pair2_word1'], row['pair2_word2']\n",
    "\n",
    "        if w1 not in vocab.stoi or w2 not in vocab.stoi or w3 not in vocab.stoi or w4 not in vocab.stoi:\n",
    "            continue\n",
    "            \n",
    "        category_id = int(row['relation1'][:-1])\n",
    "        human_score = row['mean_rating']\n",
    "        word_dict[category_id].append(human_score)\n",
    "            \n",
    "        a = vocab[w1]\n",
    "        b = vocab[w2]\n",
    "        c = vocab[w3]\n",
    "        d = vocab[w4]\n",
    "        \n",
    "        loss = get_loss(a, b, c, d)[0][0]\n",
    "        # print(loss)\n",
    "\n",
    "        prediction_dict[category_id].append(loss)\n",
    "        \n",
    "    return prediction_dict, word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorized_relsim_scores_eu(df, vocab, get_loss):\n",
    "    prediction_dict = defaultdict(lambda: [])\n",
    "    word_dict = defaultdict(lambda: [])\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        w1, w2, w3, w4 = row['pair1_word1'], row['pair1_word2'], row['pair2_word1'], row['pair2_word2']\n",
    "\n",
    "        if w1 not in vocab.stoi or w2 not in vocab.stoi or w3 not in vocab.stoi or w4 not in vocab.stoi:\n",
    "            continue\n",
    "            \n",
    "        category_id = int(row['relation1'][:-1])\n",
    "        human_score = row['mean_rating']\n",
    "        word_dict[category_id].append(human_score)\n",
    "            \n",
    "        a = vocab[w1]\n",
    "        b = vocab[w2]\n",
    "        c = vocab[w3]\n",
    "        d = vocab[w4]\n",
    "        \n",
    "        loss = get_loss(a, b, c, d)\n",
    "        # print(loss)\n",
    "\n",
    "        prediction_dict[category_id].append(loss)\n",
    "        \n",
    "    return prediction_dict, word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlopt = optim.SGD(meta_linear.parameters(), lr=0.1)\n",
    "mm1opt = optim.SGD(meta_mlp1.parameters(), lr=0.1)\n",
    "mm2opt = optim.SGD(meta_mlp2.parameters(), lr=0.1)\n",
    "\n",
    "mlcache = meta_linear.state_dict()\n",
    "mm1cache = meta_mlp1.state_dict()\n",
    "mm2cache = meta_mlp2.state_dict()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def get_ml_loss(a, b, c, d):\n",
    "    meta_linear.load_state_dict(mlcache)\n",
    "    mlopt.zero_grad()\n",
    "    a_hat = meta_linear(a)\n",
    "    loss = criterion(a_hat, b)\n",
    "    loss.backward()\n",
    "    mlopt.step()\n",
    "    c_hat = meta_linear(c)\n",
    "    loss = criterion(c_hat, d)\n",
    "    return loss.detach().numpy()\n",
    "\n",
    "def get_m1_loss(a, b, c, d):\n",
    "    meta_mlp1.load_state_dict(mm1cache)\n",
    "    mm1opt.zero_grad()\n",
    "    a_hat = meta_mlp1(a)\n",
    "    loss = criterion(a_hat, b)\n",
    "    loss.backward()\n",
    "    mm1opt.step()\n",
    "    c_hat = meta_mlp1(c)\n",
    "    loss = criterion(c_hat, d)\n",
    "    return loss.detach().numpy()\n",
    "\n",
    "def get_m2_loss(a, b, c, d):\n",
    "    meta_mlp2.load_state_dict(mm2cache)\n",
    "    mm2opt.zero_grad()\n",
    "    a_hat = meta_mlp2(a)\n",
    "    loss = criterion(a_hat, b)\n",
    "    loss.backward()\n",
    "    mm2opt.step()\n",
    "    c_hat = meta_mlp2(c)\n",
    "    loss = criterion(c_hat, d)\n",
    "    return loss.detach().numpy()\n",
    "\n",
    "def cos(a, b, c, d):\n",
    "    return sklearn.metrics.pairwise.cosine_similarity((a-b).reshape(1, -1), c-d.reshape(1, -1))\n",
    "\n",
    "def euclid(a, b, c, d):\n",
    "    r1 = a-b\n",
    "    r2 = c-d\n",
    "    return 1-np.linalg.norm(r1-r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_1samp(x_mlp2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x_mlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in m2_pred_dict.items():\n",
    "    print(key, scipy.stats.ttest_1samp(val, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6194it [00:40, 152.79it/s]\n"
     ]
    }
   ],
   "source": [
    "m2_pred_dict, m2_word_dict = get_categorized_relsim_scores(df, vocab, get_m2_loss, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6194it [00:24, 251.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.360320991824765"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = compute_alpha(df, vocab, get_m2_loss)\n",
    "np.mean(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6194it [00:03, 1993.93it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_pred, word_dict = get_categorized_relsim_scores_cos(df, vocab, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6194it [00:01, 6110.38it/s]\n"
     ]
    }
   ],
   "source": [
    "eu_predict, word_dict = get_categorized_relsim_scores_eu(df, vocab, euclid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_coeffs(word_dict, pred_dict):\n",
    "    r_list = [0] * (len(word_dict)+1)\n",
    "\n",
    "    for key, val in word_dict.items():\n",
    "        # val = np.array(val)\n",
    "        # normalized_val = (val - 1) / 6 \n",
    "        pred = pred_dict[key]\n",
    "        # pred = np.exp(np.array(pred))\n",
    "        corr, pval = scipy.stats.pearsonr(val, pred)\n",
    "        r_list[key] = corr\n",
    "        \n",
    "    return r_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(cos_dict, pred_dict):\n",
    "    all_cos_preds = []\n",
    "    all_my_preds = []\n",
    "    for key, val in cos_dict.items():\n",
    "        all_cos_preds += val\n",
    "        all_my_preds += pred_dict[key]\n",
    "        \n",
    "    return all_cos_preds, all_my_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09898973312207526, 5.7965350232696985e-15)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cos, all_m2 = get_all_preds(word_dict, m2_pred_dict)\n",
    "scipy.stats.pearsonr(all_cos, all_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_corr = get_category_coeffs(word_dict, cos_pred)\n",
    "eu_corr = get_category_coeffs(word_dict, eu_predict)\n",
    "meta_corr = get_category_coeffs(word_dict, m2_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAEGCAYAAAA61G1JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdp0lEQVR4nO3de5xXdb3v8dcbJEeBGLfWpIyn4VFaCiMgo+hRaPCKl0RLCUIfkhtnZ5q3nUfa7tTtrn288PCQ6dlFqNBJHC+pkVJmBmqlbi4CiljexhxFQ7xSYqCf88dvwR6G+cH8Ztb6XYb38/GYB+v2W9/P+jG8WdfvUkRgZmbd16vUBZiZ9RQOVDOzlDhQzcxS4kA1M0uJA9XMLCU7lLqArOy2225RV1dX6jLMrIdZvHjxGxHxiY7m9dhAraurY9GiRaUuw8x6GEkv5ZvnQ34zs5Q4UM3MUlIWgSpprKQ/SnpO0tQO5k+WtFrS0uRnSinqNDPbmpKfQ5XUG7gBOBJoBRZKmhsRT7db9LaIOKc7ba1fv57W1lbWrVvXndVYmamqqqK2tpY+ffqUuhTbzpU8UIEDgeci4gUASc3AOKB9oHZba2sr/fv3p66uDklpr95KICJYs2YNra2tDBo0qNTl2HauHA75BwIvtxlvTaa192VJyyXdKWnPrjS0bt06dt11V4dpDyKJXXfd1UcdVhbKYQ+1M34B3BoRH0j6J2A2cFj7hSQ1AU0ANTU1LFiwYLP5AwYMYO3atdlXa0W3bt26Lf6+zYqtHAL1FaDtHmdtMm2TiFjTZnQmcHVHK4qIGcAMgIaGhmhsbNxs/sqVK+nfv3/3K7ayU1VVxfDhw0tdhm3nyiFQFwJ7SRpELkgnAF9tu4Ck3SNiVTJ6ArAyjYbrpt6Xxmo2abnyuG0u89prr3H++eezcOFCqqurqampYfr06ey9994FtXXssccyZ84cqquru1puXq+++irnnnsud955Z6c/c+mllzJ69GiOOOIIGhsbmTZtGg0NDV36/PTp02lqamLnnXfuSvkVa2u/j5353bLSK3mgRsQGSecA9wO9gZsiYoWkK4BFETEXOFfSCcAG4E1gcskK7oaI4KSTTuL000+nubkZgGXLlvH6668XHKjz5s3LokQA9thjj4LCFOCKK67ocnsffvjhZp+fPn06p5566nYXqFb5yuGiFBExLyL2jojPRMT3kmmXJmFKRHw7IgZHxNCIGBMRz5S24q6ZP38+ffr04etf//qmaUOHDmXUqFFEBBdddBFDhgyhvr6e2267DYBVq1YxevRohg0bxpAhQ3jkkUeA3KO1b7zxBi0tLeyzzz6ceeaZDB48mKOOOor3338fgOeff56xY8cyYsQIRo0axTPPbPm1PfTQQwwbNoxhw4YxfPhw3nvvPVpaWhgyZAgAs2bN4sQTT+TII4+krq6O66+/nmuvvZbhw4dz0EEH8eabbwIwefLkDkP4rLPOoqGhgcGDB3PZZZdtml5XV8fFF1/M/vvvzx133LHp89dddx2vvvoqY8aMYcyYMdx0002cf/75mz734x//mAsuuKC7fxVmmSiLQN1ePPXUU4wYMaLDeXfddRdLly5l2bJl/OY3v+Giiy5i1apVzJkzh6OPPnrTvGHDhm3x2WeffZazzz6bFStWUF1dzc9+9jMAmpqa+MEPfsDixYuZNm0a3/jGN7b47LRp07jhhhtYunQpjzzyCDvttFOHdd91110sXLiQSy65hJ133pknnniCgw8+mJ/85Cdb3ebvfe97LFq0iOXLl/PQQw+xfPnyTfN23XVXlixZwoQJEzZNO/fcc9ljjz2YP38+8+fPZ/z48fziF79g/fr1ANx8882cccYZW23TrFRKfshvOb/73e+YOHEivXv3pqamhi984QssXLiQAw44gDPOOIP169dz4okndhiogwYN2jR9xIgRtLS0sHbtWv7whz9wyimnbFrugw8+2OKzhxxyCBdeeCGTJk3iS1/6ErW1tVssM2bMGPr370///v0ZMGAAX/ziFwGor6/fLCA7cvvttzNjxgw2bNjAqlWrePrpp9lvv/0A+MpXvrLN76Vfv34cdthh3Hvvveyzzz6sX7+e+vr6bX7OrBS8h1pEgwcPZvHixQV9ZvTo0Tz88MMMHDiQyZMnd7hHuOOOO24a7t27Nxs2bOCjjz6iurqapUuXbvpZuXLLa3lTp05l5syZvP/++xxyyCEdnhZou/5evXptGu/VqxcbNmzIW/uLL77ItGnTePDBB1m+fDnHHXfcZveL9u3bt1PfwZQpU5g1axY333wzX/va1zr1GbNScKAW0WGHHcYHH3zAjBkzNk1bvnw5jzzyCKNGjeK2227jww8/ZPXq1Tz88MMceOCBvPTSS9TU1HDmmWcyZcoUlixZ0qm2Pv7xjzNo0CDuuOMOIHdBbNmyZVss9/zzz1NfX8/FF1/MAQcc0GGgdtW7775L3759GTBgAK+//jq//OUvO/W5/v378957720aHzlyJC+//DJz5sxh4sSJqdVnlrbt+pC/2LeiSOLuu+/m/PPP56qrrqKqqoq6ujqmT5/OoYceyqOPPsrQoUORxNVXX82nPvUpZs+ezTXXXEOfPn3o16/fNs9ZtnXLLbdw1lln8d3vfpf169czYcIEhg4dutky06dPZ/78+fTq1YvBgwdzzDHHsGrVqjxrLMzQoUMZPnw4n//859lzzz055JBDOvW5pqYmxo4du+lcKsD48eNZunQpu+yySyq1mWVBEVHqGjLR0NAQ7TuYXrlyJfvss0+JKrLuOP7447ngggs4/PDDO5zfE/5ufR9qZZC0OCI6vMnah/xW1t5++2323ntvdtppp7xhalYututDfit/1dXV/OlPfyp1GWad4j1UM7OUOFDNzFLiQDUzS4kD1cwsJdv3RanLB6S8vne2uYgkJk2axE9/+lMANmzYwO67787IkSO59957mTVrFosWLeL666/f7HN1dXUsWrSI3XbbLd2azSw13kMtsr59+/LUU09t6hHqgQceYODAjt74YmaVxoFaAsceeyz33Ze7ifvWW2/145RmPYQDtQQmTJhAc3Mz69atY/ny5YwcObLUJZlZChyoJbDffvvR0tLCrbfeyrHHHlvqcswsJdv3RakSOuGEE/jWt77FggULWLNmzbY/YGZlz4FaImeccQbV1dXU19f79cdmPcT2HaiduM0pK7W1tZx77rkdzps1axb33HPPpvHHHnsMyJ0q6NUrd5Zm/PjxXHvttdkXamadtn0HagmsXbt2i2mNjY00NjYCuZfdTZ48eYtlWlpasi3MzLrNF6XMzFLiQDUzS8l2F6g99Q0F2zP/nVq52K4CtaqqijVr1vgfYA8SEaxZs4aqqqpSl2JWHhelJI0Fvg/0BmZGxJV5lvsycCdwQEQs6miZramtraW1tZXVq1d3q14rL1VVVdTW1pa6DLPSB6qk3sANwJFAK7BQ0tyIeLrdcv2B84DHu9pWnz59GDRoUHfKNTPLqxwO+Q8EnouIFyLi70AzMK6D5f4duApYV8zizMw6q+R7qMBA4OU2463AZr2FSNof2DMi7pN0Ub4VSWoCmgBqamr8BJJVlH+u35B3nn+XK0M5BOpWSeoFXAtM3tayETEDmAHQ0NAQG2+WN6sEk6fel3dey6TG4hViXVYOh/yvAHu2Ga9Npm3UHxgCLJDUAhwEzJXUULQKzcw6oRwCdSGwl6RBkj4GTADmbpwZEe9ExG4RURcRdcBjwAlducpvZpalkgdqRGwAzgHuB1YCt0fECklXSDqhtNWZmXVeWZxDjYh5wLx20y7Ns2xjMWoyMytUyfdQzcx6CgeqmVlKHKhmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoJqZpWSHUhcAIGks8H2gNzAzIq5sN//rwNnAh8BaoCkini56oWaWnssHbGXeO8WrI0Ul30OV1Bu4ATgG2BeYKGnfdovNiYj6iBgGXA1cW+Qyzcy2qeSBChwIPBcRL0TE34FmYFzbBSLi3TajfYEoYn1mZp1SDof8A4GX24y3AiPbLyTpbOBC4GPAYR2tSFIT0ARQU1PDggUL0q7VLDP/XL8h77we+bv8uX/LP69Ct7ccArVTIuIG4AZJXwX+FTi9g2VmADMAGhoaorGxsag1mnXH5Kn35Z3XMqmxeIUUy+Xj8s+b6HOoXfUKsGeb8dpkWj7NwImZVmRm1gXlsIe6ENhL0iByQToB+GrbBSTtFRHPJqPHAc9iZhWhLs+ed0tVkQspgpIHakRskHQOcD+526ZuiogVkq4AFkXEXOAcSUcA64G36OBw38ys1EoeqAARMQ+Y127apW2Gzyt6UWZmBSqHc6hmZj2CA9XMLCUOVDOzlHQqUCX1kvQvWRdjZlbJOhWoEfERcHzGtZiZVbRCDvmXS7pMkk8TmJl1oJDbpv4B+AJwlqTHgeXA8oi4I5PKzMwqTKcDNSLGA0jaERgM1JPrKcqBamZGF27sj4gPgCXJj3VRvsfxAFquPK6IlZhZWnw+1MwsJQ5UM7OUlMWz/Ga2DT3w/Us9Uaf3UCWdIql/Mvyvku6StH92pZmZVZZCDvm/ExHvSToUOAK4EfjPbMoyM6s8hQTqh8mfxwEzIuI+cu93MjMzCjuH+oqkHwFHAlcl96P2qItavpXJzLqjkEAcT65X/aMj4m1yT05dlElVZmYVaJt7qJJeBAJYHRGbXu8cEauAVRnWZmZWUbYZqBExqBiFmJlVuh51DtTMrJQ6fVEquQj1ZaCu7eci4or0yzIzqzyFXOX/OfAOsBj4IJtyzMwqVyGBWhsRYzOrxMyswhVyDvUPkuozq8TMrMIVsod6KDA5uY3qA0BARMR+mVRmZlZhCgnUY7IqQtJY4PtAb2BmRFzZbv6FwBRgA7AaOCMiXsqqHjOzrijkFSgvSRoKjEomPRIRy7pbgKTewA3kHmltBRZKmhsRT7dZ7AmgISL+Juks4GrgK91t2yqXHxO2clRI933nAbcAn0x+firpmynUcCDwXES8EBF/B5qBcW0XiIj5EfG3ZPQxoDaFds3MUqWI6NyC0nLg4Ij4azLeF3i0u+dQJZ0MjI2IKcn4acDIiDgnz/LXA69FxHc7mNcENAHU1NSMaG5uLqiWJ1/J31Fv/cCtdPDbBcVsqyfqid/fVrep14v5P7j7sAyqSU++7arUbRozZsziiGjoaF4h51DFf3fhRzKs7hRWKEmnAg3kXme9hYiYAcwAaGhoiMbGxoLWP3lrh5GTCltXObXVE/XE72+r21R1Wf4PTizvHvvzbVclb1M+hQTqzcDjku4mF6TjyHUy3V2vAHu2Ga9Npm1G0hHAJcAXkjevmpmVlUIuSl0raQG526cCmBwRS1OoYSGwl6RB5IJ0AvDVtgtIGg78iNypgb+k0KaZWeoKeqcU8GxEXAdUA5cmQdctEbEBOIdcX6srgdsjYoWkKySdkCx2DdAPuEPSUklzu9uumVnaCjnk/05E3JG8U+owYBrwQ2Dk1j+2bRExD5jXbtqlbYaP6G4bZmZZ6+o7pX7sd0qZmW2ukEDd+E6prwDzeuI7pczMuqNTgShJwDfwO6XMzPLq1DnUiAhJ90VEfZtpfqeUmVkbhRyyL5F0QGaVmJlVuEKu8o8EJkl6Cfgr7r7PzGwzhQTq0ZlVYWbWAxTafd8uwF5AVZtZ7pfUtu3yrXRYcnllPrdt1l4hbz2dApxH7ln7pcBBwKPkbvI3MytfRfoPvZCLUucBBwAvRcQYYDjwdmqVmJlVuEICdV1ErAOQtGNEPAN8LpuyzMwqTyEXpVolVQP3AA9IegufPzUz26SQi1InJYOXS5oPDAB+lUlVZmYVqJDu+yTpVEmXRsRD5C5Mle97CszMiqyQc6j/FzgYmJiMv0fubaVmZkaBT0pFxP6SngCIiLckufs+M7NEIXuo6yX1Jvf6EyR9Avgok6rMzCpQIYF6HXA38ElJ3wN+B/xHJlWZmVWgQq7y3yJpMXA4uY5RToyIlZlVZmZWYbYZqJKqgK8DnwWeBH6UvFjPzMza6Mwh/2yggVyYHkPu5XxmZtZOZw75993YU7+kG4H/yrYkMyupfB2JuFewberMHur6jQM+1Dczy68ze6hDJb2bDAvYKRnf2GP/xzOrzsysk+qm3pd3XktV3lmp2magRkTvrIuQNBb4PtAbmBkRV7abPxqYDuwHTIiIO7OuyawnK4fw6YkKuQ81E8nDAjeQu+C1LzBR0r7tFvszMBmYU9zqzMw6r5BHT7NyIPBcRLwAIKkZGAc8vXGBiGhJ5vnJLDMrW4qI0hYgnQyMjYgpyfhp5PoNOKeDZWcB9+Y75JfUBDQB1NTUjGhubi6olidfyX8Vs37gVl6h0AXFbKssrFqaf97uhXda1hO/v61uU68X838w7e8vX1tdaGdrbZXFNnWhrTFjxiyOiIaO5pXDHmpqImIGMAOgoaEhGhsbC/r85K2dV5pU2LrKqa2ycPm4/PMmFn47zla/v2fztFXmt/1sdZuqLsv/wbS/v3xtdaGdrbVVFtvUxbbyKfk5VOAVYM8247XJNDOzilIOgboQ2EvSoKQ7wAnA3BLXZGZWsJIHavKwwDnA/cBK4PaIWCHpCkknAEg6QFIrcArwI0krSlexmVnHyuIcakTMA+a1m3Zpm+GF5E4FmJmVrZLvoZqZ9RQOVDOzlDhQzcxS4kA1M0uJA9XMLCUOVDOzlDhQzcxS4kA1M0uJA9XMLCUOVDOzlDhQzcxS4kA1M0uJA9XMLCUOVDOzlDhQzcxS4kA1M0uJA9XMLCUOVDOzlDhQzcxS4kA1M0uJA9XMLCVl8dZTy1bd1Ps6nN5y5XFFrsSsZ/MeqplZShyoZmYpcaCamaWkLAJV0lhJf5T0nKSpHczfUdJtyfzHJdUVv0ozs60reaBK6g3cABwD7AtMlLRvu8X+EXgrIj4L/B/gquJWaWa2bSUPVOBA4LmIeCEi/g40A+PaLTMOmJ0M3wkcLklFrNHMbJsUEaUtQDoZGBsRU5Lx04CREXFOm2WeSpZpTcafT5Z5o926moAmgJqamhHNzc3pFbpqacfTdx+WXhvFbitfO11s68lX3sk7r37ggILXV/ZS/v6sMowZM2ZxRDR0NK9H3YcaETOAGQANDQ3R2NiY3sovb7/TnJiYP0TKvq187XSxrcl57ncFaJnUWPD6yl7K359VvnII1FeAPduM1ybTOlqmVdIOwABgTXHK68Eu9z96szSVwznUhcBekgZJ+hgwAZjbbpm5wOnJ8MnAb6PU5yrMzNop+R5qRGyQdA5wP9AbuCkiVki6AlgUEXOBG4H/J+k54E1yoWtmVlZKHqgAETEPmNdu2qVthtcBpxS7LjOzQpTDIb+ZWY9QFnuoZhXJF/WsHe+hmpmlxIFqZpYSB6qZWUocqGZmKXGgmpmlxIFqZpYS3zZVjnw7jllF8h6qmVlKHKhmZinxIb+lpuXK40pdgllJeQ/VzCwlDlQzs5Q4UM3MUuJANTNLiQPVzCwlDlQzs5Q4UM3MUuJANTNLiQPVzCwlDlQzs5Q4UM3MUuJANTNLSUkDVdI/SHpA0rPJn7vkWe5Xkt6WdG+xazQz66xS76FOBR6MiL2AB5PxjlwDnFa0qszMuqDUgToOmJ0MzwZO7GihiHgQeK9YRZmZdUWp+0OtiYhVyfBrQE13ViapCWgCqKmpYcGCBd2rrq3Gn3c8Pc02zKyiZR6okn4DfKqDWZe0HYmIkBTdaSsiZgAzABoaGqKxsbE7qzMzK0jmgRoRR+SbJ+l1SbtHxCpJuwN/yboeM7OslPoc6lzg9GT4dCDPcbWZWfkrdaBeCRwp6VngiGQcSQ2SZm5cSNIjwB3A4ZJaJR1dkmrNzLaipBelImINcHgH0xcBU9qMjypmXWZmXVHqPVQzsx7DgWpmlhIHqplZShyoZmYpUUS37qUvW5JWAy+luMrdgDdSXF85tNUTt6mYbfXEbSpmW5W6TZ+OiE90NKPHBmraJC2KiIae1FZP3KZittUTt6mYbfXEbfIhv5lZShyoZmYpcaB23owe2FZP3KZittUTt6mYbfW4bfI5VDOzlHgP1cwsJQ5UM7OUOFC3QdJNkv4i6amM29lT0nxJT0taIem8DNuqkvRfkpYlbf1bVm0l7fWW9ETWL1mU1CLpSUlLJS3KuK1qSXdKekbSSkkHZ9TO55Lt2fjzrqTzM2rrguT34SlJt0qqyqKdpK3zknZWpL09Hf2b7ewLQbstIvyzlR9gNLA/8FTG7ewO7J8M9wf+BOybUVsC+iXDfYDHgYMy3LYLgTnAvRl/hy3AbkX6vZgNTEmGPwZUF6HN3uReFfTpDNY9EHgR2CkZvx2YnNF2DAGeAnYm1+Pdb4DPprj+Lf7NAlcDU5PhqcBVWWyb91C3ISIeBt4sQjurImJJMvwesJLcL3kWbUVErE1G+yQ/mVydlFQLHAfM3NaylULSAHL/aG8EiIi/R8TbRWj6cOD5iEjzCcC2dgB2krQDubB7NaN29gEej4i/RcQG4CHgS2mtPM+/2U69ELS7HKhlSFIdMJzcnmNWbfSWtJTca2ceiIis2poO/C/go4zW31YAv5a0OHlhY1YGAauBm5NTGTMl9c2wvY0mALdmseKIeAWYBvwZWAW8ExG/zqItcnunoyTtKmln4Fhgz4za2ijVF4Lm40AtM5L6AT8Dzo+Id7NqJyI+jIhhQC1woKQhabch6XjgLxGxOO1153FoROwPHAOcLWl0Ru3sQO6Q8j8jYjjwV3KHkZmR9DHgBHJvrshi/buQ24sbBOwB9JV0ahZtRcRK4Crg18CvgKXAh1m0laf9IKMjMgdqGZHUh1yY3hIRdxWjzeRQdT4wNoPVHwKcIKkFaAYOk/TTDNoBNu1lERF/Ae4GDsyoqVagtc1e/Z3kAjZLxwBLIuL1jNZ/BPBiRKyOiPXAXcD/zKgtIuLGiBgREaOBt8hdM8jS68mLQMnyhaAO1DIhSeTOya2MiGszbusTkqqT4Z2AI4Fn0m4nIr4dEbURUUfucPW3EZHJXo+kvpL6bxwGjiJ3aJm6iHgNeFnS55JJhwNPZ9FWGxPJ6HA/8WfgIEk7J7+Lh5M7j58JSZ9M/vwf5M6fzsmqrURRXgha0ndKVQJJtwKNwG6SWoHLIuLGDJo6BDgNeDI5twnwLxExL4O2dgdmS+pN7j/V2yMi01uaiqAGuDuXBewAzImIX2XY3jeBW5JD8ReAr2XVUPIfxJHAP2XVRkQ8LulOYAmwAXiCbB/X/JmkXYH1wNlpXtTr6N8suReA3i7pH8l16zk+rfY2azu5jcDMzLrJh/xmZilxoJqZpcSBamaWEgeqmVlKHKhmZilxoFrFkfQpSc2Snk8eM50nae88y1ZL+kaxa7TtkwPVKkpy0/ndwIKI+ExEjAC+Tf5ns6uBzAM16VDEtnMOVKs0Y4D1EfHDjRMiYhnwhKQHJS1J+kQdl8y+EvhM0pfoNQCSLpK0UNLytn3BSvqOpD9K+l3SH+i3kunDJD2WLH/3xr40JS2QND3pe/USSS8mjw8j6eNtx2374P9VrdIMATrqbGUdcFJEvCtpN+AxSXPJdVoyJOkIBklHAXuRe85fwNykE5X3gS8DQ8l1Z7ikTTs/Ab4ZEQ9JuoLckzcbO0X+WCTve096CTsOuIfco7Z3Jc/F23bCgWo9hYD/SMLxI3J9yXZ0GuCo5OeJZLwfuYDtD/w8ItYB6yT9Ajb1fVodEQ8ly89m8x6fbmszPJNcV4X3kHsU9cwUtssqiAPVKs0K4OQOpk8CPgGMiIj1SQ9XHb3CQ8D/jogfbTax66/h+OvGgYj4vaQ6SY1A74jI9LU5Vn58DtUqzW+BHdt2IC1pP+DT5PpeXS9pTDIO8B65vc+N7gfOSPqdRdLApOej3wNfVO59W/2A4wEi4h3gLUmjks+fRq6H+Xx+Qq7npJu7uZ1WgbyHahUlIkLSScB0SReTO3faAlwOXCfpSWARSXeEEbFG0u+TF7b9MiIukrQP8GjSM9Va4NSIWJicc10OvA48CbyTNHs68MOkd/lt9Sx1C/Bdsu1qz8qUe5syS0jqFxFrk+B8GGja+J6vAtZxMjAuIk7LpEgra95DNftvMyTtS+7c6+wuhOkPyPWsf2wWxVn58x6qmVlKfFHKzCwlDlQzs5Q4UM3MUuJANTNLiQPVzCwl/x+5MZKJekPQqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ind = np.arange(10)\n",
    "width = 0.25\n",
    "p1 = ax.bar(ind, cos_corr, width)\n",
    "# p2 = ax.bar(ind+width, eu_corr, width)\n",
    "p3 = ax.bar(ind+width, meta_corr, width)\n",
    "\n",
    "# ax.set_title(\"Pearson's $r$ by category\")\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels((1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "ax.legend((p1[0], p3[0]), ('Cosine similarity',\n",
    "                                 'MLL'))\n",
    "ax.set_ylabel(\"Pearson's $r$\")\n",
    "ax.autoscale_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pred_dict[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array(word_dict[3][:10])\n",
    "(vals-1)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
